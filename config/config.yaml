artifacts_root: artifacts


data_ingestion:
  root_dir: artifacts/data_ingestion
  source_URL: https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip
  local_data_file: artifacts/data_ingestion/data.zip
  unzip_dir : artifacts/data_ingestion


data_validation:
  root_dir: artifacts/data_validation
  STATUS_FILE: artifacts/data_validation/status.txt
  ALL_REQUIRED_FILES: ["train" , "test" , "validation"]
  data_dir: artifacts/data_ingestion/samsum_dataset


data_transformation:
  root_dir: artifacts/data_transformation
  data_path: artifacts/data_ingestion/samsum_dataset
  tokenizer_name: google/pegasus-cnn_dailymail
  dev_run: false
  dev_model: null


model_trainer:
  root_dir: artifacts/model_trainer
  data_path: artifacts/data_transformation/samsum_dataset
  model_ckpt: google/pegasus-cnn_dailymail
  # ── Quick smoke-test mode ─────────────────────────────────────────────────
  # Set dev_run: true to train on a tiny slice of data so you can verify the
  # full pipeline end-to-end in minutes instead of hours.
  # Switch back to false (and dev_subset: 0) for a real training run.
  dev_run: true
  dev_model: null          # null = use model_ckpt above
  dev_subset: 40           # 40 train samples, 10 validation samples


model_evaluation:
  root_dir: artifacts/model_evaluation
  data_path: artifacts/data_transformation/samsum_dataset
  model_path: artifacts/model_trainer/pegasus-samsum-model
  tokenizer_path: artifacts/model_trainer/tokenizer
  metric_file_name: artifacts/model_evaluation/metrics.csv
  # Fallback HuggingFace Hub model used when local fine-tuned model is absent
  # (e.g. on HuggingFace Spaces before training). Replace with your own
  # fine-tuned model ID once you push it to the Hub.
  hub_model_id: google/pegasus-cnn_dailymail